{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1OTi7nuO7xc1SglLV7tqk_5kwx4VFxd7p",
      "authorship_tag": "ABX9TyPUt56YvTHPjlfB1Tvk3119",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JoyNjihia/WEEK-4-ASSIGNMENT-AI-FOR-SOFTWARE-ENGINEERING/blob/main/task4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "task 4"
      ],
      "metadata": {
        "id": "2Cj3F8C9Fzec"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "id": "v4GV0HjcFwq7",
        "outputId": "477c45f9-2a32-4366-ab74-844acaf51a38"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Could not convert ['MMMMMMMMMMMMMMMMMMMBBBMMMMMMMMMMMMMMMBMMMMMMMMBMBBBBBMMBMMBBBBMBMMBBBBMBMMBMBMMBBBMMBMMMBBBMBBMMBBBMMBBBBMBBMBBBBBBBBMMMBMMBBBMMBMBMMBMMBBMBBMBBBBMBBBBBBBBBMBBBBMMBMBBMMBBMMBBBBMBBMMMBMBMBBBMBBMMBMMMMBMMMBMBMBBMBMMMMBBMMBBBMBBBBBMMBBMBBMMBMBBBBMBBBBBMBMMMMMMMMMMMMMMBBBBBBMBMBBMBBMBMMBBBBBBBBBBBBBMBBMBMBBBBBBBBBBBBBBMBBBMBMBBBBMMMBBBBMBMBMBBBMBBBBBBBMMMBBBBBBBBBBBMMBMMMBMMBBBBBMBBBBBMBBBMBBMMBBBBBBMBBBBBBBMBBBBBMBBMBBBBBBBBBBBBMBMMBMBBBBBMBBMBMBBMBMBBBBBBBBMMBBBBBBMBBBBBBBBBBMBBBBBBBMBMBBMBBBBBMMBMBMBBBBBMBBMBMBMMBBBMBBBBBBBBBBBMBMMBBBBBBBBBBBBBBBBBBBBBBBBBMMMMMMB'] to numeric",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4-2336191791.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Handle missing values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Encode categorical variables (if any)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(self, axis, skipna, numeric_only, **kwargs)\u001b[0m\n\u001b[1;32m  11691\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  11692\u001b[0m     ):\n\u001b[0;32m> 11693\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumeric_only\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m  11694\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  11695\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"mean\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(self, axis, skipna, numeric_only, **kwargs)\u001b[0m\n\u001b[1;32m  12418\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  12419\u001b[0m     ) -> Series | float:\n\u001b[0;32m> 12420\u001b[0;31m         return self._stat_function(\n\u001b[0m\u001b[1;32m  12421\u001b[0m             \u001b[0;34m\"mean\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnanops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnanmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumeric_only\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  12422\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_stat_function\u001b[0;34m(self, name, func, axis, skipna, numeric_only, **kwargs)\u001b[0m\n\u001b[1;32m  12375\u001b[0m         \u001b[0mvalidate_bool_kwarg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"skipna\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnone_allowed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  12376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 12377\u001b[0;31m         return self._reduce(\n\u001b[0m\u001b[1;32m  12378\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumeric_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnumeric_only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  12379\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_reduce\u001b[0;34m(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\u001b[0m\n\u001b[1;32m  11560\u001b[0m         \u001b[0;31m# After possibly _get_data and transposing, we are now in the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  11561\u001b[0m         \u001b[0;31m#  simple case where we can use BlockManager.reduce\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 11562\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblk_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m  11563\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor_from_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  11564\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mout_dtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"boolean\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mreduce\u001b[0;34m(self, func)\u001b[0m\n\u001b[1;32m   1498\u001b[0m         \u001b[0mres_blocks\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mBlock\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1499\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mblk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1500\u001b[0;31m             \u001b[0mnbs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1501\u001b[0m             \u001b[0mres_blocks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mreduce\u001b[0;34m(self, func)\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mblk_func\u001b[0;34m(values, axis)\u001b[0m\n\u001b[1;32m  11479\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  11480\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 11481\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m  11482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  11483\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36mf\u001b[0;34m(values, axis, skipna, **kwds)\u001b[0m\n\u001b[1;32m    145\u001b[0m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(values, axis, skipna, mask, **kwargs)\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdatetimelike\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36mnanmean\u001b[0;34m(values, axis, skipna, mask)\u001b[0m\n\u001b[1;32m    718\u001b[0m     \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m     \u001b[0mthe_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype_sum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 720\u001b[0;31m     \u001b[0mthe_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ensure_numeric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthe_sum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthe_sum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ndim\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36m_ensure_numeric\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1684\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minferred\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"string\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"mixed\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1685\u001b[0m                 \u001b[0;31m# GH#44008, GH#36703 avoid casting e.g. strings to numeric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1686\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Could not convert {x} to numeric\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1687\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1688\u001b[0m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomplex128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Could not convert ['MMMMMMMMMMMMMMMMMMMBBBMMMMMMMMMMMMMMMBMMMMMMMMBMBBBBBMMBMMBBBBMBMMBBBBMBMMBMBMMBBBMMBMMMBBBMBBMMBBBMMBBBBMBBMBBBBBBBBMMMBMMBBBMMBMBMMBMMBBMBBMBBBBMBBBBBBBBBMBBBBMMBMBBMMBBMMBBBBMBBMMMBMBMBBBMBBMMBMMMMBMMMBMBMBBMBMMMMBBMMBBBMBBBBBMMBBMBBMMBMBBBBMBBBBBMBMMMMMMMMMMMMMMBBBBBBMBMBBMBBMBMMBBBBBBBBBBBBBMBBMBMBBBBBBBBBBBBBBMBBBMBMBBBBMMMBBBBMBMBMBBBMBBBBBBBMMMBBBBBBBBBBBMMBMMMBMMBBBBBMBBBBBMBBBMBBMMBBBBBBMBBBBBBBMBBBBBMBBMBBBBBBBBBBBBMBMMBMBBBBBMBBMBMBBMBMBBBBBBBBMMBBBBBBMBBBBBBBBBBMBBBBBBBMBMBBMBBBBBMMBMBMBBBBBMBBMBMBMMBBBMBBBBBBBBBBBMBMMBBBBBBBBBBBBBBBBBBBBBBBBBMMMMMMB'] to numeric"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Load the dataset\n",
        "# Note: Ensure the dataset is downloaded and placed in the correct directory\n",
        "# The dataset can be loaded from Kaggle: https://www.kaggle.com/datasets/uciml/breast-cancer-wisconsin-diagnostic-dataset\n",
        "df = pd.read_csv('breast-cancer-wisconsin-data.csv')\n",
        "\n",
        "# Data preprocessing\n",
        "# Drop the 'id' column which is not relevant\n",
        "df = df.drop('id', axis=1)\n",
        "\n",
        "# Handle missing values\n",
        "df.fillna(df.mean(), inplace=True)\n",
        "\n",
        "# Encode categorical variables (if any)\n",
        "# In this case, 'diagnosis' is the target column, and it's categorical\n",
        "# Convert the target variable to binary (0 for 'B', 1 for 'M') if needed\n",
        "\n",
        "# Split features and target\n",
        "X = df.drop('diagnosis', axis=1)\n",
        "y = df['diagnosis']\n",
        "\n",
        "# Convert 'diagnosis' to binary (0 for 'B', 1 for 'M') if needed\n",
        "y = y.apply(lambda x: 1 if x == 'M' else 0)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Feature scaling (optional but recommended for many machine learning models)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Model training with Random Forest\n",
        "# Train the model\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"F1 Score:\", f1_score(y_test, y_pred, average='weighted'))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Hyperparameter tuning (optional)\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'min_samples_split': [2, 5],\n",
        "    'min_samples_leaf': [1, 2]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(model, param_grid, cv=5, n_jobs=-1, verbose=2)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"\\nBest Parameters:\", grid_search.best_params_)\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Predict with the best model\n",
        "y_pred_best = best_model.predict(X_test)\n",
        "print(\"\\nBest Model Accuracy:\", accuracy_score(y_test, y_pred_best))\n",
        "print(\"\\nBest Model F1 Score:\", f1_score(y_test, y_pred_best, average='weighted'))\n",
        "\n",
        "# Feature importance analysis\n",
        "feature_importances = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Importance': model.feature_importances_\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print(\"\\nFeature Importances:\")\n",
        "print(feature_importances)\n",
        "\n",
        "# Handle class imbalance (optional)\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "model_res = RandomForestClassifier(random_state=42)\n",
        "model_res.fit(X_train_res, y_train_res)\n",
        "y_pred_res = model_res.predict(X_test)\n",
        "\n",
        "print(\"\\nAccuracy after SMOTE:\", accuracy_score(y_test, y_pred_res))\n",
        "print(\"\\nF1 Score after SMOTE:\", f1_score(y_test, y_pred_res, average='weighted'))\n",
        "\n",
        "# Save the model (optional)\n",
        "import joblib\n",
        "joblib.dump(best_model, 'breast_cancer_model.pkl')"
      ]
    },
    {
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Load the dataset\n",
        "# Note: Ensure the dataset is downloaded and placed in the correct directory\n",
        "# The dataset can be loaded from Kaggle: https://www.kaggle.com/datasets/uciml/breast-cancer-wisconsin-diagnostic-dataset\n",
        "df = pd.read_csv('breast-cancer-wisconsin-data.csv')\n",
        "\n",
        "# Data preprocessing\n",
        "# Drop the 'id' column which is not relevant\n",
        "df = df.drop('id', axis=1)\n",
        "\n",
        "# Handle missing values\n",
        "# Calculate the mean only for numeric columns\n",
        "numeric_cols = df.select_dtypes(include=np.number).columns\n",
        "df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].mean())\n",
        "\n",
        "# Encode categorical variables (if any)\n",
        "# In this case, 'diagnosis' is the target column, and it's categorical\n",
        "# Convert the target variable to binary (0 for 'B', 1 for 'M') if needed\n",
        "\n",
        "# Split features and target\n",
        "X = df.drop('diagnosis', axis=1)\n",
        "y = df['diagnosis']\n",
        "\n",
        "# Convert 'diagnosis' to binary (0 for 'B', 1 for 'M') if needed\n",
        "y = y.apply(lambda x: 1 if x == 'M' else 0)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Feature scaling (optional but recommended for many machine learning models)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Model training with Random Forest\n",
        "# Train the model\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"F1 Score:\", f1_score(y_test, y_pred, average='weighted'))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Hyperparameter tuning (optional)\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'min_samples_split': [2, 5],\n",
        "    'min_samples_leaf': [1, 2]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(model, param_grid, cv=5, n_jobs=-1, verbose=2)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"\\nBest Parameters:\", grid_search.best_params_)\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Predict with the best model\n",
        "y_pred_best = best_model.predict(X_test)\n",
        "print(\"\\nBest Model Accuracy:\", accuracy_score(y_test, y_pred_best))\n",
        "print(\"\\nBest Model F1 Score:\", f1_score(y_test, y_pred_best, average='weighted'))\n",
        "\n",
        "# Feature importance analysis\n",
        "feature_importances = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Importance': model.feature_importances_\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print(\"\\nFeature Importances:\")\n",
        "print(feature_importances)\n",
        "\n",
        "# Handle class imbalance (optional)\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "model_res = RandomForestClassifier(random_state=42)\n",
        "model_res.fit(X_train_res, y_train_res)\n",
        "y_pred_res = model_res.predict(X_test)\n",
        "\n",
        "print(\"\\nAccuracy after SMOTE:\", accuracy_score(y_test, y_pred_res))\n",
        "print(\"\\nF1 Score after SMOTE:\", f1_score(y_test, y_pred_res, average='weighted'))\n",
        "\n",
        "# Save the model (optional)\n",
        "import joblib\n",
        "joblib.dump(best_model, 'breast_cancer_model.pkl')"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ybogkko4HBN0",
        "outputId": "90db881c-e52c-4e28-acb6-87f8df6890b9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/extmath.py:1101: RuntimeWarning: invalid value encountered in divide\n",
            "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/extmath.py:1106: RuntimeWarning: invalid value encountered in divide\n",
            "  T = new_sum / new_sample_count\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/extmath.py:1126: RuntimeWarning: invalid value encountered in divide\n",
            "  new_unnormalized_variance -= correction**2 / new_sample_count\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9649122807017544\n",
            "F1 Score: 0.9647382344750765\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.99      0.97        71\n",
            "           1       0.98      0.93      0.95        43\n",
            "\n",
            "    accuracy                           0.96       114\n",
            "   macro avg       0.97      0.96      0.96       114\n",
            "weighted avg       0.97      0.96      0.96       114\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[70  1]\n",
            " [ 3 40]]\n",
            "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
            "\n",
            "Best Parameters: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
            "\n",
            "Best Model Accuracy: 0.9649122807017544\n",
            "\n",
            "Best Model F1 Score: 0.9647382344750765\n",
            "\n",
            "Feature Importances:\n",
            "                    Feature  Importance\n",
            "27     concave points_worst    0.157233\n",
            "23               area_worst    0.125468\n",
            "7       concave points_mean    0.121484\n",
            "22          perimeter_worst    0.118191\n",
            "20             radius_worst    0.079864\n",
            "3                 area_mean    0.058493\n",
            "6            concavity_mean    0.047589\n",
            "0               radius_mean    0.034461\n",
            "13                  area_se    0.034259\n",
            "2            perimeter_mean    0.028303\n",
            "26          concavity_worst    0.025581\n",
            "21            texture_worst    0.018426\n",
            "12             perimeter_se    0.018266\n",
            "1              texture_mean    0.017201\n",
            "25        compactness_worst    0.015277\n",
            "10                radius_se    0.014004\n",
            "24         smoothness_worst    0.013811\n",
            "28           symmetry_worst    0.010887\n",
            "5          compactness_mean    0.009141\n",
            "19     fractal_dimension_se    0.006669\n",
            "4           smoothness_mean    0.006057\n",
            "29  fractal_dimension_worst    0.005327\n",
            "14            smoothness_se    0.005028\n",
            "16             concavity_se    0.004893\n",
            "8             symmetry_mean    0.004783\n",
            "15           compactness_se    0.004633\n",
            "11               texture_se    0.004123\n",
            "17        concave points_se    0.003913\n",
            "18              symmetry_se    0.003460\n",
            "9    fractal_dimension_mean    0.003175\n",
            "30              Unnamed: 32    0.000000\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Input X contains NaN.\nSMOTE does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-5-2457353443.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;31m# Handle class imbalance (optional)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0msmote\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSMOTE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m \u001b[0mX_train_res\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmote\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_resample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0mmodel_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/imblearn/base.py\u001b[0m in \u001b[0;36mfit_resample\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    200\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mcorresponding\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mX_resampled\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \"\"\"\n\u001b[0;32m--> 202\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_resample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_more_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/imblearn/base.py\u001b[0m in \u001b[0;36mfit_resample\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0marrays_transformer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mArraysTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinarize_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         self.sampling_strategy_ = check_sampling_strategy(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/imblearn/base.py\u001b[0m in \u001b[0;36m_check_X_y\u001b[0;34m(self, X, y, accept_sparse)\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0maccept_sparse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinarize_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_target_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindicate_one_vs_all\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinarize_y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mvalidate_data\u001b[0;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[1;32m   2959\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2960\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2961\u001b[0;31m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2962\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1368\u001b[0m     \u001b[0mensure_all_finite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deprecate_force_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforce_all_finite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_all_finite\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m     X = check_array(\n\u001b[0m\u001b[1;32m   1371\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m         \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mensure_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1107\u001b[0;31m             _assert_all_finite(\n\u001b[0m\u001b[1;32m   1108\u001b[0m                 \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m                 \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m     _assert_all_finite_element_wise(\n\u001b[0m\u001b[1;32m    121\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mxp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    167\u001b[0m                 \u001b[0;34m\"#estimators-that-handle-nan-values\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             )\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input X contains NaN.\nSMOTE does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
          ]
        }
      ]
    },
    {
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.impute import SimpleImputer # Import SimpleImputer\n",
        "\n",
        "# Load the dataset\n",
        "# Note: Ensure the dataset is downloaded and placed in the correct directory\n",
        "# The dataset can be loaded from Kaggle: https://www.kaggle.com/datasets/uciml/breast-cancer-wisconsin-diagnostic-dataset\n",
        "df = pd.read_csv('breast-cancer-wisconsin-data.csv')\n",
        "\n",
        "# Data preprocessing\n",
        "# Drop the 'id' column which is not relevant\n",
        "df = df.drop('id', axis=1)\n",
        "\n",
        "# Handle missing values\n",
        "# Calculate the mean only for numeric columns\n",
        "numeric_cols = df.select_dtypes(include=np.number).columns\n",
        "df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].mean())\n",
        "\n",
        "# Encode categorical variables (if any)\n",
        "# In this case, 'diagnosis' is the target column, and it's categorical\n",
        "# Convert the target variable to binary (0 for 'B', 1 for 'M') if needed\n",
        "\n",
        "# Split features and target\n",
        "X = df.drop('diagnosis', axis=1)\n",
        "y = df['diagnosis']\n",
        "\n",
        "# Convert 'diagnosis' to binary (0 for 'B', 1 for 'M') if needed\n",
        "y = y.apply(lambda x: 1 if x == 'M' else 0)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Feature scaling (optional but recommended for many machine learning models)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Model training with Random Forest\n",
        "# Train the model\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"F1 Score:\", f1_score(y_test, y_pred, average='weighted'))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Hyperparameter tuning (optional)\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'min_samples_split': [2, 5],\n",
        "    'min_samples_leaf': [1, 2]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(model, param_grid, cv=5, n_jobs=-1, verbose=2)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"\\nBest Parameters:\", grid_search.best_params_)\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Predict with the best model\n",
        "y_pred_best = best_model.predict(X_test)\n",
        "print(\"\\nBest Model Accuracy:\", accuracy_score(y_test, y_pred_best))\n",
        "print(\"\\nBest Model F1 Score:\", f1_score(y_test, y_pred_best, average='weighted'))\n",
        "\n",
        "# Feature importance analysis\n",
        "feature_importances = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Importance': model.feature_importances_\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print(\"\\nFeature Importances:\")\n",
        "print(feature_importances)\n",
        "\n",
        "# Handle class imbalance (optional)\n",
        "\n",
        "# Apply imputation *after* scaling and *before* SMOTE\n",
        "imputer = SimpleImputer(strategy='mean') # Or 'median', 'most_frequent', etc.\n",
        "X_train_imputed = imputer.fit_transform(X_train)\n",
        "\n",
        "smote = SMOTE(random_state=42)\n",
        "# Use the imputed training data for SMOTE\n",
        "X_train_res, y_train_res = smote.fit_resample(X_train_imputed, y_train)\n",
        "\n",
        "\n",
        "model_res = RandomForestClassifier(random_state=42)\n",
        "model_res.fit(X_train_res, y_train_res)\n",
        "y_pred_res = model_res.predict(X_test)\n",
        "\n",
        "print(\"\\nAccuracy after SMOTE:\", accuracy_score(y_test, y_pred_res))\n",
        "print(\"\\nF1 Score after SMOTE:\", f1_score(y_test, y_pred_res, average='weighted'))\n",
        "\n",
        "# Save the model (optional)\n",
        "import joblib\n",
        "joblib.dump(best_model, 'breast_cancer_model.pkl')"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yKsBneKPHqYC",
        "outputId": "a80c8cc7-855b-413a-a931-bca7b5d045e3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/extmath.py:1101: RuntimeWarning: invalid value encountered in divide\n",
            "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/extmath.py:1106: RuntimeWarning: invalid value encountered in divide\n",
            "  T = new_sum / new_sample_count\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/extmath.py:1126: RuntimeWarning: invalid value encountered in divide\n",
            "  new_unnormalized_variance -= correction**2 / new_sample_count\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9649122807017544\n",
            "F1 Score: 0.9647382344750765\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.99      0.97        71\n",
            "           1       0.98      0.93      0.95        43\n",
            "\n",
            "    accuracy                           0.96       114\n",
            "   macro avg       0.97      0.96      0.96       114\n",
            "weighted avg       0.97      0.96      0.96       114\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[70  1]\n",
            " [ 3 40]]\n",
            "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
            "\n",
            "Best Parameters: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
            "\n",
            "Best Model Accuracy: 0.9649122807017544\n",
            "\n",
            "Best Model F1 Score: 0.9647382344750765\n",
            "\n",
            "Feature Importances:\n",
            "                    Feature  Importance\n",
            "27     concave points_worst    0.157233\n",
            "23               area_worst    0.125468\n",
            "7       concave points_mean    0.121484\n",
            "22          perimeter_worst    0.118191\n",
            "20             radius_worst    0.079864\n",
            "3                 area_mean    0.058493\n",
            "6            concavity_mean    0.047589\n",
            "0               radius_mean    0.034461\n",
            "13                  area_se    0.034259\n",
            "2            perimeter_mean    0.028303\n",
            "26          concavity_worst    0.025581\n",
            "21            texture_worst    0.018426\n",
            "12             perimeter_se    0.018266\n",
            "1              texture_mean    0.017201\n",
            "25        compactness_worst    0.015277\n",
            "10                radius_se    0.014004\n",
            "24         smoothness_worst    0.013811\n",
            "28           symmetry_worst    0.010887\n",
            "5          compactness_mean    0.009141\n",
            "19     fractal_dimension_se    0.006669\n",
            "4           smoothness_mean    0.006057\n",
            "29  fractal_dimension_worst    0.005327\n",
            "14            smoothness_se    0.005028\n",
            "16             concavity_se    0.004893\n",
            "8             symmetry_mean    0.004783\n",
            "15           compactness_se    0.004633\n",
            "11               texture_se    0.004123\n",
            "17        concave points_se    0.003913\n",
            "18              symmetry_se    0.003460\n",
            "9    fractal_dimension_mean    0.003175\n",
            "30              Unnamed: 32    0.000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: [30]. At least one non-missing value is needed for imputation with strategy='mean'.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "X has 31 features, but RandomForestClassifier is expecting 30 features as input.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-6-1833294839.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0mmodel_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0mmodel_res\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_res\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_res\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m \u001b[0my_pred_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_res\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nAccuracy after SMOTE:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_res\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m         \"\"\"\n\u001b[0;32m--> 904\u001b[0;31m         \u001b[0mproba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    905\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    944\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m         \u001b[0;31m# Check data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 946\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m         \u001b[0;31m# Assign chunk of trees to jobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    636\u001b[0m             \u001b[0mensure_all_finite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         X = validate_data(\n\u001b[0m\u001b[1;32m    639\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mvalidate_data\u001b[0;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[1;32m   2963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2964\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcheck_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ensure_2d\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2965\u001b[0;31m         \u001b[0m_check_n_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_estimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2966\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2967\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_check_n_features\u001b[0;34m(estimator, X, reset)\u001b[0m\n\u001b[1;32m   2827\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features_in_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   2830\u001b[0m             \u001b[0;34mf\"X has {n_features} features, but {estimator.__class__.__name__} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m             \u001b[0;34mf\"is expecting {estimator.n_features_in_} features as input.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: X has 31 features, but RandomForestClassifier is expecting 30 features as input."
          ]
        }
      ]
    },
    {
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.impute import SimpleImputer # Import SimpleImputer\n",
        "\n",
        "# Load the dataset\n",
        "# Note: Ensure the dataset is downloaded and placed in the correct directory\n",
        "# The dataset can be loaded from Kaggle: https://www.kaggle.com/datasets/uciml/breast-cancer-wisconsin-diagnostic-dataset\n",
        "df = pd.read_csv('breast-cancer-wisconsin-data.csv')\n",
        "\n",
        "# Data preprocessing\n",
        "# Drop the 'id' column which is not relevant\n",
        "df = df.drop('id', axis=1)\n",
        "\n",
        "# Handle missing values\n",
        "# Calculate the mean only for numeric columns\n",
        "numeric_cols = df.select_dtypes(include=np.number).columns\n",
        "df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].mean())\n",
        "\n",
        "# Encode categorical variables (if any)\n",
        "# In this case, 'diagnosis' is the target column, and it's categorical\n",
        "# Convert the target variable to binary (0 for 'B', 1 for 'M') if needed\n",
        "\n",
        "# Split features and target\n",
        "X = df.drop('diagnosis', axis=1)\n",
        "y = df['diagnosis']\n",
        "\n",
        "# Convert 'diagnosis' to binary (0 for 'B', 1 for 'M') if needed\n",
        "y = y.apply(lambda x: 1 if x == 'M' else 0)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Feature scaling (optional but recommended for many machine learning models)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test) # Apply the *same* scaling to test set\n",
        "\n",
        "# Handle class imbalance (optional)\n",
        "\n",
        "# Apply imputation *after* scaling and *before* SMOTE\n",
        "# Fit the imputer *only* on the scaled training data\n",
        "imputer = SimpleImputer(strategy='mean') # Or 'median', 'most_frequent', etc.\n",
        "X_train_imputed = imputer.fit_transform(X_train_scaled)\n",
        "\n",
        "# Transform the scaled test data using the *fitted* imputer\n",
        "X_test_imputed = imputer.transform(X_test_scaled)\n",
        "\n",
        "# Now apply SMOTE to the imputed training data\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_res, y_train_res = smote.fit_resample(X_train_imputed, y_train)\n",
        "\n",
        "# Model training with Random Forest after SMOTE\n",
        "model_res = RandomForestClassifier(random_state=42)\n",
        "# Train the model on the resampled and imputed training data\n",
        "model_res.fit(X_train_res, y_train_res)\n",
        "\n",
        "# Predict on the *imputed* test set\n",
        "y_pred_res = model_res.predict(X_test_imputed)\n",
        "\n",
        "# Evaluate the model after SMOTE\n",
        "print(\"\\nAccuracy after SMOTE:\", accuracy_score(y_test, y_pred_res))\n",
        "print(\"\\nF1 Score after SMOTE:\", f1_score(y_test, y_pred_res, average='weighted'))\n",
        "\n",
        "# The remaining code for model evaluation, tuning, etc. should now work correctly\n",
        "# as it uses the properly scaled and imputed data where needed.\n",
        "\n",
        "# Model training with Random Forest (original, before SMOTE) - Keep this if you still want to evaluate the model without SMOTE\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "# Train the model on the scaled but *not* imputed or resampled training data (assuming no NaNs were introduced by scaling)\n",
        "# Note: If scaling introduced NaNs, you would need to impute X_train_scaled before fitting this model too.\n",
        "# Since your original non-SMOTE part worked, it implies scaling didn't create NaNs in the numeric columns.\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict on the scaled test set (assuming no NaNs introduced by scaling)\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the model (original)\n",
        "print(\"\\nAccuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"F1 Score:\", f1_score(y_test, y_pred, average='weighted'))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Hyperparameter tuning (optional) - Use the scaled and imputed training data for grid search\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'min_samples_split': [2, 5],\n",
        "    'min_samples_leaf': [1, 2]\n",
        "}\n",
        "\n",
        "# Fit GridSearchCV on the resampled and imputed training data\n",
        "grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5, n_jobs=-1, verbose=2)\n",
        "grid_search.fit(X_train_res, y_train_res)\n",
        "\n",
        "\n",
        "print(\"\\nBest Parameters:\", grid_search.best_params_)\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Predict with the best model on the imputed test set\n",
        "y_pred_best = best_model.predict(X_test_imputed)\n",
        "print(\"\\nBest Model Accuracy:\", accuracy_score(y_test, y_pred_best))\n",
        "print(\"\\nBest Model F1 Score:\", f1_score(y_test, y_pred_best, average='weighted'))\n",
        "\n",
        "# Feature importance analysis - Use the model trained on scaled data (before SMOTE/imputation if you want feature names)\n",
        "# Note: Feature importances on SMOTE'd data might be less interpretable with original feature names.\n",
        "# If using X_train_imputed for training, the columns might be different if imputation dropped one.\n",
        "# Let's use the model trained on X_train_scaled (31 features) if it fitted without error.\n",
        "# If you want importances from the SMOTE'd model, you'd need to map back or check column names of X_train_imputed.\n",
        "# Given the original model fit X_train_scaled (31 features), let's use that.\n",
        "feature_importances = pd.DataFrame({\n",
        "    # X.columns are the original column names (31 features)\n",
        "    'Feature': X.columns,\n",
        "    # Use importances from the model trained on 31 features\n",
        "    'Importance': model.feature_importances_\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print(\"\\nFeature Importances:\")\n",
        "print(feature_importances)\n",
        "\n",
        "\n",
        "# Save the model (optional)\n",
        "import joblib\n",
        "# Save the best model found from the grid search on the resampled data\n",
        "joblib.dump(best_model, 'breast_cancer_model_resampled.pkl')\n",
        "# You might also want to save the imputer and scaler if you need to preprocess new data for prediction\n",
        "joblib.dump(scaler, 'scaler.pkl')\n",
        "joblib.dump(imputer, 'imputer.pkl')"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8Q0-vdMIFHD",
        "outputId": "c7c163a4-46c4-42b7-b136-5966c507d585"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/extmath.py:1101: RuntimeWarning: invalid value encountered in divide\n",
            "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/extmath.py:1106: RuntimeWarning: invalid value encountered in divide\n",
            "  T = new_sum / new_sample_count\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/extmath.py:1126: RuntimeWarning: invalid value encountered in divide\n",
            "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: [30]. At least one non-missing value is needed for imputation with strategy='mean'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: [30]. At least one non-missing value is needed for imputation with strategy='mean'.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy after SMOTE: 0.9649122807017544\n",
            "\n",
            "F1 Score after SMOTE: 0.9649122807017544\n",
            "\n",
            "Accuracy: 0.9649122807017544\n",
            "F1 Score: 0.9647382344750765\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.99      0.97        71\n",
            "           1       0.98      0.93      0.95        43\n",
            "\n",
            "    accuracy                           0.96       114\n",
            "   macro avg       0.97      0.96      0.96       114\n",
            "weighted avg       0.97      0.96      0.96       114\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[70  1]\n",
            " [ 3 40]]\n",
            "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
            "\n",
            "Best Parameters: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
            "\n",
            "Best Model Accuracy: 0.9649122807017544\n",
            "\n",
            "Best Model F1 Score: 0.9649122807017544\n",
            "\n",
            "Feature Importances:\n",
            "                    Feature  Importance\n",
            "27     concave points_worst    0.157233\n",
            "23               area_worst    0.125468\n",
            "7       concave points_mean    0.121484\n",
            "22          perimeter_worst    0.118191\n",
            "20             radius_worst    0.079864\n",
            "3                 area_mean    0.058493\n",
            "6            concavity_mean    0.047589\n",
            "0               radius_mean    0.034461\n",
            "13                  area_se    0.034259\n",
            "2            perimeter_mean    0.028303\n",
            "26          concavity_worst    0.025581\n",
            "21            texture_worst    0.018426\n",
            "12             perimeter_se    0.018266\n",
            "1              texture_mean    0.017201\n",
            "25        compactness_worst    0.015277\n",
            "10                radius_se    0.014004\n",
            "24         smoothness_worst    0.013811\n",
            "28           symmetry_worst    0.010887\n",
            "5          compactness_mean    0.009141\n",
            "19     fractal_dimension_se    0.006669\n",
            "4           smoothness_mean    0.006057\n",
            "29  fractal_dimension_worst    0.005327\n",
            "14            smoothness_se    0.005028\n",
            "16             concavity_se    0.004893\n",
            "8             symmetry_mean    0.004783\n",
            "15           compactness_se    0.004633\n",
            "11               texture_se    0.004123\n",
            "17        concave points_se    0.003913\n",
            "18              symmetry_se    0.003460\n",
            "9    fractal_dimension_mean    0.003175\n",
            "30              Unnamed: 32    0.000000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['imputer.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    }
  ]
}