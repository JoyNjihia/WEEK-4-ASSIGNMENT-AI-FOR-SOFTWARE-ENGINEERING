{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "11U5h_usPlX-Bl1VsUJklI5_8GC6IVM38",
      "authorship_tag": "ABX9TyNjff3PRR/rrHdQsn0MIXNW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JoyNjihia/WEEK-4-ASSIGNMENT-AI-FOR-SOFTWARE-ENGINEERING/blob/main/task4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUUwGBWoKCgr",
        "outputId": "126e4bd5-4134-4636-c599-fb81226c4edc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/extmath.py:1101: RuntimeWarning: invalid value encountered in divide\n",
            "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/extmath.py:1106: RuntimeWarning: invalid value encountered in divide\n",
            "  T = new_sum / new_sample_count\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/extmath.py:1126: RuntimeWarning: invalid value encountered in divide\n",
            "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: [30]. At least one non-missing value is needed for imputation with strategy='mean'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: [30]. At least one non-missing value is needed for imputation with strategy='mean'.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy after SMOTE: 0.9649122807017544\n",
            "\n",
            "F1 Score after SMOTE: 0.9649122807017544\n",
            "\n",
            "Accuracy: 0.9649122807017544\n",
            "F1 Score: 0.9647382344750765\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.99      0.97        71\n",
            "           1       0.98      0.93      0.95        43\n",
            "\n",
            "    accuracy                           0.96       114\n",
            "   macro avg       0.97      0.96      0.96       114\n",
            "weighted avg       0.97      0.96      0.96       114\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[70  1]\n",
            " [ 3 40]]\n",
            "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
            "\n",
            "Best Parameters: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
            "\n",
            "Best Model Accuracy: 0.9649122807017544\n",
            "\n",
            "Best Model F1 Score: 0.9649122807017544\n",
            "\n",
            "Feature Importances:\n",
            "                    Feature  Importance\n",
            "27     concave points_worst    0.157233\n",
            "23               area_worst    0.125468\n",
            "7       concave points_mean    0.121484\n",
            "22          perimeter_worst    0.118191\n",
            "20             radius_worst    0.079864\n",
            "3                 area_mean    0.058493\n",
            "6            concavity_mean    0.047589\n",
            "0               radius_mean    0.034461\n",
            "13                  area_se    0.034259\n",
            "2            perimeter_mean    0.028303\n",
            "26          concavity_worst    0.025581\n",
            "21            texture_worst    0.018426\n",
            "12             perimeter_se    0.018266\n",
            "1              texture_mean    0.017201\n",
            "25        compactness_worst    0.015277\n",
            "10                radius_se    0.014004\n",
            "24         smoothness_worst    0.013811\n",
            "28           symmetry_worst    0.010887\n",
            "5          compactness_mean    0.009141\n",
            "19     fractal_dimension_se    0.006669\n",
            "4           smoothness_mean    0.006057\n",
            "29  fractal_dimension_worst    0.005327\n",
            "14            smoothness_se    0.005028\n",
            "16             concavity_se    0.004893\n",
            "8             symmetry_mean    0.004783\n",
            "15           compactness_se    0.004633\n",
            "11               texture_se    0.004123\n",
            "17        concave points_se    0.003913\n",
            "18              symmetry_se    0.003460\n",
            "9    fractal_dimension_mean    0.003175\n",
            "30              Unnamed: 32    0.000000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['imputer.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.impute import SimpleImputer # Import SimpleImputer\n",
        "\n",
        "# Load the dataset\n",
        "# Note: Ensure the dataset is downloaded and placed in the correct directory\n",
        "# The dataset can be loaded from Kaggle: https://www.kaggle.com/datasets/uciml/breast-cancer-wisconsin-diagnostic-dataset\n",
        "df = pd.read_csv('breast-cancer-wisconsin-data.csv')\n",
        "\n",
        "# Data preprocessing\n",
        "# Drop the 'id' column which is not relevant\n",
        "df = df.drop('id', axis=1)\n",
        "\n",
        "# Handle missing values\n",
        "# Calculate the mean only for numeric columns\n",
        "numeric_cols = df.select_dtypes(include=np.number).columns\n",
        "df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].mean())\n",
        "\n",
        "# Encode categorical variables (if any)\n",
        "# In this case, 'diagnosis' is the target column, and it's categorical\n",
        "# Convert the target variable to binary (0 for 'B', 1 for 'M') if needed\n",
        "\n",
        "# Split features and target\n",
        "X = df.drop('diagnosis', axis=1)\n",
        "y = df['diagnosis']\n",
        "\n",
        "# Convert 'diagnosis' to binary (0 for 'B', 1 for 'M') if needed\n",
        "y = y.apply(lambda x: 1 if x == 'M' else 0)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Feature scaling (optional but recommended for many machine learning models)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test) # Apply the *same* scaling to test set\n",
        "\n",
        "# Handle class imbalance (optional)\n",
        "\n",
        "# Apply imputation *after* scaling and *before* SMOTE\n",
        "# Fit the imputer *only* on the scaled training data\n",
        "imputer = SimpleImputer(strategy='mean') # Or 'median', 'most_frequent', etc.\n",
        "X_train_imputed = imputer.fit_transform(X_train_scaled)\n",
        "\n",
        "# Transform the scaled test data using the *fitted* imputer\n",
        "X_test_imputed = imputer.transform(X_test_scaled)\n",
        "\n",
        "# Now apply SMOTE to the imputed training data\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_res, y_train_res = smote.fit_resample(X_train_imputed, y_train)\n",
        "\n",
        "# Model training with Random Forest after SMOTE\n",
        "model_res = RandomForestClassifier(random_state=42)\n",
        "# Train the model on the resampled and imputed training data\n",
        "model_res.fit(X_train_res, y_train_res)\n",
        "\n",
        "# Predict on the *imputed* test set\n",
        "y_pred_res = model_res.predict(X_test_imputed)\n",
        "\n",
        "# Evaluate the model after SMOTE\n",
        "print(\"\\nAccuracy after SMOTE:\", accuracy_score(y_test, y_pred_res))\n",
        "print(\"\\nF1 Score after SMOTE:\", f1_score(y_test, y_pred_res, average='weighted'))\n",
        "\n",
        "# The remaining code for model evaluation, tuning, etc. should now work correctly\n",
        "# as it uses the properly scaled and imputed data where needed.\n",
        "\n",
        "# Model training with Random Forest (original, before SMOTE) - Keep this if you still want to evaluate the model without SMOTE\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "# Train the model on the scaled but *not* imputed or resampled training data (assuming no NaNs were introduced by scaling)\n",
        "# Note: If scaling introduced NaNs, you would need to impute X_train_scaled before fitting this model too.\n",
        "# Since your original non-SMOTE part worked, it implies scaling didn't create NaNs in the numeric columns.\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict on the scaled test set (assuming no NaNs introduced by scaling)\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the model (original)\n",
        "print(\"\\nAccuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"F1 Score:\", f1_score(y_test, y_pred, average='weighted'))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Hyperparameter tuning (optional) - Use the scaled and imputed training data for grid search\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'min_samples_split': [2, 5],\n",
        "    'min_samples_leaf': [1, 2]\n",
        "}\n",
        "\n",
        "# Fit GridSearchCV on the resampled and imputed training data\n",
        "grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5, n_jobs=-1, verbose=2)\n",
        "grid_search.fit(X_train_res, y_train_res)\n",
        "\n",
        "\n",
        "print(\"\\nBest Parameters:\", grid_search.best_params_)\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Predict with the best model on the imputed test set\n",
        "y_pred_best = best_model.predict(X_test_imputed)\n",
        "print(\"\\nBest Model Accuracy:\", accuracy_score(y_test, y_pred_best))\n",
        "print(\"\\nBest Model F1 Score:\", f1_score(y_test, y_pred_best, average='weighted'))\n",
        "\n",
        "# Feature importance analysis - Use the model trained on scaled data (before SMOTE/imputation if you want feature names)\n",
        "# Note: Feature importances on SMOTE'd data might be less interpretable with original feature names.\n",
        "# If using X_train_imputed for training, the columns might be different if imputation dropped one.\n",
        "# Let's use the model trained on X_train_scaled (31 features) if it fitted without error.\n",
        "# If you want importances from the SMOTE'd model, you'd need to map back or check column names of X_train_imputed.\n",
        "# Given the original model fit X_train_scaled (31 features), let's use that.\n",
        "feature_importances = pd.DataFrame({\n",
        "    # X.columns are the original column names (31 features)\n",
        "    'Feature': X.columns,\n",
        "    # Use importances from the model trained on 31 features\n",
        "    'Importance': model.feature_importances_\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print(\"\\nFeature Importances:\")\n",
        "print(feature_importances)\n",
        "\n",
        "\n",
        "# Save the model (optional)\n",
        "import joblib\n",
        "# Save the best model found from the grid search on the resampled data\n",
        "joblib.dump(best_model, 'breast_cancer_model_resampled.pkl')\n",
        "# You might also want to save the imputer and scaler if you need to preprocess new data for prediction\n",
        "joblib.dump(scaler, 'scaler.pkl')\n",
        "joblib.dump(imputer, 'imputer.pkl')"
      ]
    }
  ]
}